{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "5000\n"
     ]
    }
   ],
   "source": [
    "#extract data\n",
    "# this part of code is taken from tensorflow tutorial\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "print(len(mnist.validation.labels))\n",
    "trX, trY,vdX,vdY, teX, teY = mnist.train.images, mnist.train.labels,mnist.validation.images, mnist.validation.labels, mnist.test.images, mnist.test.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clamp(s):\n",
    "    return tf.clip_by_value(s, 0.,1.)\n",
    "# Glorot/Bengio weight initialization\n",
    "# this function is copied from Benjamin Scelliers code\n",
    "def initialize_layer(n_in, n_out):\n",
    "    rng = np.random.RandomState()\n",
    "    W_values = np.asarray(\n",
    "        rng.uniform(\n",
    "            low=-np.sqrt(6. / (n_in + n_out)),\n",
    "            high=np.sqrt(6. / (n_in + n_out)),\n",
    "            size=(n_in, n_out)\n",
    "        ),\n",
    "        dtype=np.float32\n",
    "    )\n",
    "    return W_values\n",
    "\n",
    "def rho(s):\n",
    "    return tf.sigmoid(s)\n",
    "def clampByNumpy(s):\n",
    "    return np.clip(s,0.,1.)\n",
    "def rhoByNumpy(s):\n",
    "    return 1./(1+np.exp(-s))\n",
    "def save_params(path, biases, weights, training_curves):\n",
    "        f = file(path, 'wb')\n",
    "        to_dump        = biases, weights, training_curves\n",
    "        cPickle.dump(to_dump, f, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def free_phase_energy(inputLayer, hiddenLayer, outputLayer, hiddenWeight, outputWeight, inputBias, hiddenBias, outputBias):\n",
    "    squared_term= tf.reduce_sum(tf.square(inputLayer))+tf.reduce_sum(tf.square(hiddenLayer))+tf.reduce_sum(tf.square(outputLayer))\n",
    "    biased_term= tf.reduce_sum(tf.multiply(inputLayer, inputBias))+tf.reduce_sum(tf.multiply(hiddenLayer, hiddenBias))+tf.reduce_sum(tf.multiply(outputLayer, outputBias))\n",
    "    weighted_term1= tf.reduce_sum(tf.multiply(tf.matmul(tf.reshape(inputLayer,[1,784]),hiddenWeight),rho(hiddenLayer)))\n",
    "    weighted_term2=tf.reduce_sum(tf.multiply(tf.matmul(tf.reshape(rho(hiddenLayer),[1,500]), outputWeight),rho(outputLayer)))\n",
    "    weighted_term= weighted_term1+weighted_term2\n",
    "    return squared_term/2.- weighted_term/2. - biased_term\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weakly_clamped_energy(inputLyer, hiddenLayer, outputLayer, hiddenWeight, outputWeight, inputBias, hiddenBias, outputBias, beta, yData):\n",
    "    #implement F= E + beta * C\n",
    "    # C= ||prediction - yData||^2\n",
    "    E= free_phase_energy(inputLyer, hiddenLayer, outputLayer, hiddenWeight, outputWeight, inputBias, hiddenBias, outputBias)\n",
    "    C= tf.reduce_sum(tf.square(tf.subtract(yData,outputLayer)))\n",
    "    F= tf.add(E, tf.multiply(beta,C))\n",
    "    return F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting epoch 1\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n",
      "100th Milestone\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-98e2084db774>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0;31m#clamp each input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0mcx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m             \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mcount\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1015\u001b[0m                 run_metadata):\n\u001b[1;32m   1016\u001b[0m       \u001b[0;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1017\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1018\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_extend_graph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1064\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1065\u001b[0m           tf_session.TF_ExtendGraph(\n\u001b[0;32m-> 1066\u001b[0;31m               self._session, graph_def.SerializeToString(), status)\n\u001b[0m\u001b[1;32m   1067\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_opened\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1068\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_epochs=10\n",
    "alphas=[0.5,0.05]\n",
    "bt=0.001\n",
    "dir_path = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "path= dir_path+os.sep+\"net1.net\"\n",
    "layer_sizes = [28*28] + [500] + [10]\n",
    "biases_values  = [np.zeros((size), dtype=np.float32) for size in layer_sizes]\n",
    "weights_values = [initialize_layer(size_pre,size_post) for size_pre,size_post in zip(layer_sizes[:-1],layer_sizes[1:])]\n",
    "training_curves = dict()\n",
    "training_curves[\"training error\"]   = list()\n",
    "training_curves[\"validation error\"] = list()\n",
    "#create tensors\n",
    "H_var= tf.Variable(np.zeros(500, dtype=np.float32),name=\"hidden_layer_variable\")\n",
    "O_var= tf.Variable(np.zeros(10, dtype=np.float32), name=\"output_layer_variable\")\n",
    "Hw_var= tf.Variable(weights_values[0], name=\"hidden_weight_variable\")\n",
    "Ow_var= tf.Variable(weights_values[1], name=\"output_weight_variable\")\n",
    "Bin_var= tf.Variable(biases_values[0], name=\"input_bias_variable\")\n",
    "Bh_var= tf.Variable(biases_values[1], name=\"hidden_bias_variable\")\n",
    "Bo_var= tf.Variable(biases_values[2], name=\"output_bias_variable\")\n",
    "# create placeholder\n",
    "H_ph= tf.placeholder(dtype=tf.float32, shape=[500])\n",
    "O_ph= tf.placeholder(dtype=tf.float32, shape=[10])\n",
    "Hw_ph=tf.placeholder(dtype=tf.float32, shape=weights_values[0].shape)\n",
    "Ow_ph=tf.placeholder(dtype=tf.float32, shape=weights_values[1].shape)\n",
    "Bin_ph= tf.placeholder(dtype=tf.float32, shape=biases_values[0].shape)\n",
    "Bh_ph= tf.placeholder(dtype=tf.float32, shape=biases_values[1].shape)\n",
    "Bo_ph= tf.placeholder(dtype=tf.float32, shape=biases_values[2].shape)\n",
    "beta= tf.placeholder(dtype=tf.float32)\n",
    "y_data= tf.placeholder(dtype=tf.float32, shape=[10])\n",
    "\n",
    "IL= tf.placeholder(dtype=tf.float32, shape=[784])\n",
    "\n",
    "energy_of_free_phase= free_phase_energy(IL, H_var, O_var,Hw_ph, Ow_ph, Bin_ph, Bh_ph, Bo_ph)\n",
    "totalEnergy= weakly_clamped_energy(IL, H_var, O_var,Hw_ph, Ow_ph, Bin_ph, Bh_ph, Bo_ph, beta, y_data)\n",
    "free_phase_optimization=tf.train.GradientDescentOptimizer(0.01).minimize(energy_of_free_phase)\n",
    "weakly_clamped_optimization= tf.train.GradientDescentOptimizer(0.01).minimize(totalEnergy)\n",
    "\n",
    "count=0\n",
    "start_time = time.clock()\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for i in range(n_epochs):\n",
    "        print(\"starting epoch \"+ str(i+1))\n",
    "        error=0.\n",
    "        for x,y in zip(trX,trY):\n",
    "            #clamp each input\n",
    "            cx=sess.run(clamp(x))\n",
    "            count =count +1\n",
    "            if count==100:\n",
    "                print(\"100th Milestone\")\n",
    "                count=0\n",
    "            #print(\"clamping done\")\n",
    "            #create layer\n",
    "            inputLayer= x\n",
    "            hiddenLayer= np.zeros(500, dtype=np.float32)\n",
    "            outputLayer= np.zeros(10, dtype=np.float32)\n",
    "            #run free phase\n",
    "            \n",
    "            sess.run(free_phase_optimization, feed_dict={IL:cx, Hw_ph:weights_values[0], Ow_ph:weights_values[1], Bin_ph:biases_values[0], Bh_ph:biases_values[1], Bo_ph:biases_values[2]})\n",
    "            # save hidden layer and output layer values\n",
    "            free_hidden_layer= sess.run(H_var)\n",
    "            free_output_layer= sess.run(O_var)\n",
    "            #print(free_output_layer)\n",
    "            sign = 2*np.random.randint(0,2)-1 # random sign +1 or -1\n",
    "            bt = np.float32(sign*bt)\n",
    "            #run weakly clamped phase\n",
    "            sess.run(weakly_clamped_optimization, feed_dict={IL:cx, Hw_ph:weights_values[0], Ow_ph:weights_values[1], Bin_ph:biases_values[0], Bh_ph:biases_values[1], Bo_ph:biases_values[2], beta:bt, y_data:y})\n",
    "            weakly_clamped_hidden_layer=sess.run(H_var)\n",
    "            weakly_clamped_output_layer=sess.run(O_var)\n",
    "            \n",
    "            #now implement the update bias and beta \n",
    "            partA= np.matmul(rhoByNumpy(cx).reshape(len(cx),1), rhoByNumpy(weakly_clamped_hidden_layer).reshape(1, len(weakly_clamped_hidden_layer)))\n",
    "            partB= np.matmul(rhoByNumpy(cx).reshape(len(cx),1), rhoByNumpy(free_hidden_layer).reshape(1, len(free_hidden_layer)))\n",
    "            delW0=-1*(partA-partB)/2.\n",
    "            \n",
    "            partC= np.matmul(rhoByNumpy(weakly_clamped_hidden_layer).reshape(len(weakly_clamped_hidden_layer),1), rhoByNumpy(weakly_clamped_output_layer).reshape(1, len(weakly_clamped_output_layer)))\n",
    "            partD= np.matmul(rhoByNumpy(free_hidden_layer).reshape(len(free_hidden_layer),1), rhoByNumpy(free_output_layer).reshape(1, len(free_output_layer)))\n",
    "            delW1= -1*(partC-partD)/2.\n",
    "            #update Weights\n",
    "            weights_values[0]=weights_values[0]- delW0*alphas[0]\n",
    "            weights_values[1]=weights_values[1]- delW1*alphas[1]\n",
    "            \n",
    "            #get delB\n",
    "            delB1= -1*(rhoByNumpy(weakly_clamped_hidden_layer)-rhoByNumpy(free_hidden_layer))\n",
    "            delB2=-1*(rhoByNumpy(weakly_clamped_output_layer)-rhoByNumpy(free_output_layer))\n",
    "            #update bias\n",
    "            biases_values[1]=biases_values[1]- alphas[0]*delB1\n",
    "            biases_values[2]=biases_values[2]- alphas[1]*delB2\n",
    "            \n",
    "            error=error+ (np.argmax(y)==np.argmax(free_output_layer))*1.\n",
    "            #print(\"image processed\")\n",
    "        training_curves[\"training error\"].append(np.average(error))\n",
    "        print(\"Epoch \"+str(i+1)+\" training error rate: \"+ str(np.average(error)))\n",
    "        #check validation set\n",
    "        #run free phase with validation data\n",
    "        verror=0.\n",
    "        for x,y in zip(vdX, vdY):\n",
    "            cx=sess.run(clamp(x))\n",
    "            hiddenLayer= np.zeros(500, dtype=np.float32)\n",
    "            outputLayer= np.zeros(10, dtype=np.float32)\n",
    "            sess.run(free_phase_optimization, feed_dict={IL:cx, Hw_ph:weights_values[0], Ow_ph:weights_values[1], Bin_ph:biases_values[0], Bh_ph:biases_values[1], Bo_ph:biases_values[2]})\n",
    "            pred= sess.run(O_var)\n",
    "            verror= verror+ (np.argmax(y)==np.argmax(pred))*1.\n",
    "        #test error\n",
    "        training_curves[\"validation error\"].append(np.average(verror))\n",
    "        print(\"Epoch \"+str(i+1)+\" validation error rate: \"+ str(np.average(verror)))\n",
    "        \n",
    "            # in free phase layer will be a variable and weights and bias will be fixed\n",
    "    \n",
    "    save_params(path, biases_values, weights_values, training_curves)\n",
    "duration = (time.clock() - start_time) / 60.\n",
    "print(\"   duration=%.1f min\" % (duration))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
